\section{Support Vector Machine (SVM)}
Die Support Vector Machine kann verwendet werden, um ein Modell für das Zuordnen von Daten zu entwickeln.
Dadurch wird ein Satz von Punkten, deren Klassifizierung bekannt ist, so linear getrennt, dass der Abstand von der trennenden Geraden zu den beiden Punktegruppen maximal wird.
Es resultiert eine einfache Gleichung, mit der sich neue Daten klassifizieren lassen.

\subsection{Linear Trennbare Daten}
\subsubsection{Allgemeines}

\myul{\textbf{Datenpunkte:}} (2D Beispiel)\\
\begin{tabular}{lllll}
    $A:(\underbrace{(x_1,x_2)}_{\vec{x}_1};y_1)$, &
    $B:(\underbrace{(x_1,x_2)}_{\vec{x}_2};y_2)$, &
    $C:(\underbrace{(x_1,x_2)}_{\vec{x}_3};y_3)$, & 
    $\cdots $, &
    $N:(\underbrace{(x_1,x_2)}_{\vec{x}_n};y_n)$\\
\end{tabular}

$\vec{x}_j$ sind Datenvektoren\\
$y_j$ $\in $ \{$\pm 1$\} klassifiziert die jeweiligen Datenvektoren
\newline
\begin{minipage}[]{0.56\columnwidth}
    \myul{\textbf{Hyperebenen:}}\\
    $\boxed{\vec{w}^{\tr}\cdot\vec{x}+b=0}$ \\
    $\vec{w}$: Normalenvektor, $\vec{w} \in \mathbb{R}^d$ und $\vec{w} \neq 0$\\
    $b$: Konstante, $b \in \mathbb{R}$\\
    Dimension der Hyperebene $= d-1$\\
    Abstand der Hyperebene zum Ursprung: $\frac{\left\lvert b \right\rvert }{\left\lvert \vec{w}\right\rvert }$
    \medskip

    \myul{\textbf{Klassifizierung:}}\\
    $\boxed{\vec{w}^{\tr}\cdot\vec{x}+b>0} \Rightarrow \vec{x} $ gehört zur Klasse $y = +1$\\
    $\boxed{\vec{w}^{\tr}\cdot\vec{x}+b<0} \Rightarrow \vec{x} $ gehört zur Klasse $y = -1$
    \medskip

    \myul{\textbf{Klassifizierung der Trainigsdaten:}}\\
    $\boxed{\vec{w}^{\tr}\cdot\vec{x}_j+b \geq 0} \Rightarrow \vec{x}_j $ gehört zur Klasse $y = +1$\\
    $\boxed{\vec{w}^{\tr}\cdot\vec{x}_j+b\leq 0} \Rightarrow \vec{x}_j $ gehört zur Klasse $y = -1$
    \medskip

    \myul{\textbf{Zielfunktion:}}\\
    $\boxed{\frac{2}{\left\lvert \vec{w}\right\rvert }=\frac{2}{w}}$
\end{minipage}\hfill
\begin{minipage}[t]{0.4\columnwidth}
    \includegraphics[width=4cm]{images/7_model_values.png}
\end{minipage}

\subsubsection{Das primale Optimierungsproblem}
$\boxed{\frac12\vec{w}^{\tr}\cdot\vec{w}=\frac12\left|\vec{w}\right|^2=\frac12w^2\to\min!\quad\mathrm{s.t.}\quad\left(\vec{w}^{\tr}\cdot\vec{x}_j+b\right)y_j\geq1\quad\left(j=1,\cdots,N\right)}$

\subsubsection{Das duale Optimierungsproblem}
\myul{\textbf{Nebenbedingung:}}\\
$\boxed{\underbrace{1-\left(\vec{w}^{\tr}\cdot\vec{x}_j+b\right)y}_{g_j(\vec{w}^{\tr},b)}\leq0\Leftrightarrow g_j(\vec{w}^{\tr},b)\leq0\quad\left(j=1,\cdots,N\right)}$
\medskip

\myul{\textbf{Lagrange-Funktion:}}\\
Zusammengesetzt aus dem primalen Problem und den Nebenbedingungen.\\
\begin{tabular}{|r@{ }l|}
    \hline
    $\mathrm{L}(\vec{w}^{\tr},b,\vec{a})$ & $ =\mathrm{L}(w_1,w_2, ...,w_d,b,\alpha_1,\alpha_2, ...,\alpha_N) $ \\
                                & $ =\frac12\vec{w}^{\tr}\cdot\vec{w}+\sum_{j=1}^N\alpha_j\left(\underbrace{1-\left(\vec{w}^{\tr}\cdot\vec{x}_j+b\right)y_j}_{g_j\left(\vec{w}^{\tr},b\right)}\right) $ \\
    \hline
\end{tabular}
\medskip

\myul{\textbf{Stationaritätsbedingungen:}}\\
Aus der Bedingung, dass $\grad(\mathrm{L}) = 0$ sein muss, lassen sich folgende Formeln ableiten:
$\boxed{\grad_{\left\{\vec{w}^{\tr},b\right\}}\left(\mathrm{L}(\vec{w}^{\tr},b,\vec{\alpha})\right)=\vec{0}} \Leftrightarrow 
\boxed{\vec{w}=\sum_{j=1}^N\alpha_jy_j\vec{x}_j} \text{ und }
\boxed{\sum_{j=1}^N\alpha_jy_j=0}$

%\myul{\textbf{Lagrange-Funktion mit den Dualvariablen $\alpha_j$:}}\\
%$\boxed{L(\vec{w}^{\tr},b,\vec{\alpha})=\sum_{j=1}^N\alpha_j-\underbrace{\frac12\sum_{j,j'=1}^N\alpha_j\alpha_j,y_jy_j,\vec{x}_j^{\tr}\cdot\vec{x}_j}_{\frac{1}{2}\vec{w}^{\tr}\cdot\vec{w}}:=L(\vec{\alpha})\quad , \alpha_j\geq0}$
\myul{\textbf{Das duale Problem:}}\\
Die oben erhaltenen Summen können nun in die Lagrange-Fkt. eingesetzt werden. Daraus entsteht\\
$\boxed{\mathrm{L}(\vec{\alpha})=\sum_{j=1}^N\alpha_j-\underbrace{\frac12\sum_{j,j'=1}^N\alpha_j\alpha_{j'} \space y_jy_{j'} \space \vec{x}_j^{\tr}\cdot\vec{x}_{j'}}_{=\frac12 \vec{w}^{\tr} \cdot \vec{w}}\quad\to\quad\max!\quad\mathrm{s.t.}\quad\alpha_j\geq0\wedge\sum_{j=1}^N\alpha_jy_j=0}$

\myul{\textbf{Vorgehen zum lösen des dualen Optimierungsproblems:}}

\begin{enumerate}
    \item \myul{\textbf{Skizze mit Datenpunkten erstellen:}} \\
    \begin{minipage}[c]{0.6\columnwidth}
        \begin{itemize}
            \item Einzelne Datenpunkte klassenweise farblich hervorheben
            \item Falls ein Datenpunkt der gleichen Klasse weit weg von den anderen ist\\
            \textrightarrow\ diesen vergessen, da sein $\alpha = 0$ sein wird
        \end{itemize}
    \end{minipage}\hfill
    \begin{minipage}[c]{0.33\columnwidth}
        \input{tikz/svm_2.tex}
        % \includegraphics[width=\columnwidth]{images/SVM_2.png}
    \end{minipage}
    \columnbreak
    \item \myul{\textbf{Nebenbedingungen, Es muss gelten:}}
    \begin{itemize}
        \item [\textbf{a:}] $\boxed{\alpha_j\geq0}$
        \item [\textbf{b:}] $\boxed{\sum_{j=1}^N\alpha_j \cdot y_j=0}$\\
        Nach einem $\alpha$ unstellen und anschliessend jenes $\alpha$\\(damit die Nebenbedingung miteinbezogen wird) in der Lagrange-Funktion ersetzen
    \end{itemize}
    \item \myul{\textbf{Kernel-Matrix aufstellen:}}\\
        $\boxed{K\left( \vec{x}^{\tr} ; \vec{x}\right) = \vec{x} \dotp \vec{x}}$\\
        \includegraphics[width=5cm]{images/kernel_matrix.png}
        \begin{itemize}
            \item Einträge sind die Ergebnisse der Skalarprodukte
        \end{itemize}
    \item \myul{\textbf{Lagrange-Funktion aufstellen:}}\\
        $\boxed{\mathrm{L}(\vec{\alpha})=\sum_{j=1}^N\alpha_j-\frac{1}{2}\sum_{j,j'=1}^N\alpha_j \cdot \alpha_{j'} \cdot \space y_j \cdot y_{j'} \cdot \space \vec{x}_j \dotp  \vec{x}_{j'}\quad\to\quad\max!}$
        \begin{itemize}
            \item \textbf{2. b} und \textbf{3} brauchen
        \end{itemize}
    \item \myul{\textbf{Alle $\alpha$ finden durch Stationaritätsbedingung}}\\
            $\boxed{\nabla \mathrm{L} = \vec{0}}$\\
            $\Rightarrow$ ersetztes $\alpha$ mit gefundenen $\alpha$ berechnen 
    \item \myul{\textbf{$\bm{\vec{w}}$ berechnen:}}\\
        $\boxed{\vec{w}=\sum_{j=1}^N\alpha_jy_j\vec{x}_j}$
    \item \myul{\textbf{Konstante $\mathbf{b}$ berechnen:}}\\
        Datenpunkte mit der Klasse $y = 1$ oder $y = -1$ wählen und einsetzen
        \begin{itemize}
            \item \textbf{Variante 1:} Stützvektor-Datenpunkt mit $y = +1$\\
                $\boxed{\vec{w}^{\tr}\cdot\vec{x}_{...} + b = 1} \Leftrightarrow  \boxed{b = 1 -\vec{w}^{\tr}\cdot\vec{x}_{...} = ...}$
            \item \textbf{Variante 2:} Stützvektor-Datenpunkt mit $y = -1$\\
                $\boxed{\vec{w}^{\tr}\cdot\vec{x}_{...} + b = -1} \Leftrightarrow  \boxed{b = -1 -\vec{w}^{\tr}\cdot\vec{x}_{...} = ...}$
        \end{itemize}
\end{enumerate}

\subsubsection{Euklidische Norm}
Die euklidische Norm, ist eine in der Mathematik häufig verwendete Vektornorm.
Im zwei- und dreidimensionalen euklidischen Raum entspricht die euklidische Norm der anschaulichen Länge
oder dem Betrag eines Vektors und kann mit dem Satz des Pythagoras berechnet werden.
\begin{center}
    $\boxed{\left\lvert \vec{v_d}\right\rvert = \sqrt{x^2 + y^2 + z^2 + \dots + d^2}}$    
\end{center}

\subsection{Nicht linear Trennbare Daten}
Sollte ein Datensatz von nicht linear trennbaren Datenpunkten vorliegen, so muss dieser durch eine Transformation linear trennbar gemacht werden.
Dadurch werden die Punkte oft in höhere Dimensionen gebracht.
Das Finden einer geeigneten Transformation liegt nicht im Ramen dieses Moduls.

Der einzige Unterschied zu der Methode zum linearen trennen von Datenpunkten ist dann, dass stat mit den Datenpunkten $\vec{x}_i$ mit deren durch $\varphi$ transformierten gegenstücken $\vec{\varphi}(\vec{x}_i)$ gerechnet wird.

\subsubsection{Transformiertes duales Optimierungsproblem}
\myul{\textbf{Nebenbedingungen, Es muss gelten:}}
    \begin{itemize}
        \item [\textbf{a:}] $\boxed{\alpha_j\geq0}$
        \item [\textbf{b:}] $\boxed{\sum_{j=1}^N\alpha_j \cdot y_j=0}$
    \end{itemize}
$\boxed{\mathrm{L}(\vec{\alpha})=\sum_{j=1}^N\alpha_j-\frac{1}{2}\sum_{j,j'=1}^N\alpha_j \cdot \alpha_{j'} \cdot \space y_j \cdot y_{j'} \cdot \space \underbrace{\vec{z}_j \dotp  \vec{z}_{j'}}_{\text{Kernel}} \quad\to\quad\max!}$

\subsubsection{Kernelfunktionen (\txtqt{Kernel-Trick})}
$\boxed{K\left( \vec{x}_j ; \vec{x}_{j'}\right) = \vec{z}_j \dotp \vec{z}_{j'}=\vec{\varphi}(\vec{x}_j) \dotp \vec{\varphi}(\vec{x}_{j'})}$

$\boxed{\vec{w}=\sum_{j=1}^N\alpha_j \cdot y_j \cdot \vec{\varphi}(\vec{x}_j)}$

$\boxed{\sum_{j=1}^N\alpha_j \cdot y_j \cdot K\Big(\vec{x}_j;\vec{x}^{\text{ *}} \Big)+b>0\Longrightarrow\vec{x}^{\text{ *}}}$ gehört zur Klasse $y = +1$\\
$\boxed{\sum_{j=1}^N\alpha_j \cdot y_j \cdot K\Big(\vec{x}_j;\vec{x}^{\text{ *}} \Big)+b<0\Longrightarrow\vec{x}^{\text{ *}}}$ gehört zur Klasse $y = -1$\\
$\vec{x}^*$ steht für Test-Daten

\medskip
\textcolor{gray}{Lösungsweg: gleiches Vorgehen wie beim linearen Fall.}

